# TinyZero ä»£ç åº“æ¶æ„è¯¦è§£

## ğŸ“– é¡¹ç›®æ¦‚è¿°

**TinyZero** æ˜¯ä¸€ä¸ªåŸºäº [veRL](https://github.com/volcengine/verl) (Volcano Engine Reinforcement Learning for LLM) æ¡†æ¶çš„é¡¹ç›®ï¼Œæ—¨åœ¨å¤ç° [DeepSeek R1 Zero](https://github.com/deepseek-ai/DeepSeek-R1) çš„è‡ªä¸»æ¨ç†èƒ½åŠ›è®­ç»ƒæ–¹æ³•ã€‚

### æ ¸å¿ƒç›®æ ‡
- é€šè¿‡å¼ºåŒ–å­¦ä¹  (RL) ä½¿ 3B å‚æ•°çš„åŸºåº§è¯­è¨€æ¨¡å‹è‡ªä¸»å‘å±•å‡º **è‡ªæˆ‘éªŒè¯** å’Œ **æœç´¢æ¨ç†** èƒ½åŠ›
- æä¾›ä½æˆæœ¬ï¼ˆ< $30ï¼‰çš„å®éªŒæ–¹æ¡ˆ
- æ”¯æŒ countdownï¼ˆå€’è®¡æ—¶ç®—æœ¯ï¼‰å’Œ multiplyï¼ˆä¹˜æ³•ï¼‰ä»»åŠ¡

---

## ğŸ—ï¸ æ•´ä½“æ¶æ„è®¾è®¡æ€æƒ³

### 1. HybridFlow æ··åˆç¼–ç¨‹æ¨¡å‹

veRL çš„æ ¸å¿ƒè®¾è®¡æ€æƒ³æ˜¯ **HybridFlow**ï¼Œç»“åˆäº†ä¸¤ç§åˆ†å¸ƒå¼ç¼–ç¨‹èŒƒå¼çš„ä¼˜åŠ¿ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Single Controller                           â”‚
â”‚  (Driver Process - è´Ÿè´£è°ƒåº¦å’Œè½»é‡çº§è®¡ç®—)                           â”‚
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    RayPPOTrainer                           â”‚   â”‚
â”‚  â”‚  - åˆ›å»ºæ•°æ®åŠ è½½å™¨                                           â”‚   â”‚
â”‚  â”‚  - åè°ƒå„ä¸ª WorkerGroup                                     â”‚   â”‚
â”‚  â”‚  - è®¡ç®— Advantage (è½»é‡çº§)                                  â”‚   â”‚
â”‚  â”‚  - ç®¡ç†è®­ç»ƒå¾ªç¯                                             â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â”‚ RPC è°ƒç”¨
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Multi Controller / Workers                     â”‚
â”‚  (GPU Workers - è´Ÿè´£é‡è®¡ç®—ä»»åŠ¡)                                    â”‚
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚ActorRollout  â”‚  â”‚   Critic     â”‚  â”‚   RefPolicy  â”‚            â”‚
â”‚  â”‚   Worker     â”‚  â”‚   Worker     â”‚  â”‚   Worker     â”‚            â”‚
â”‚  â”‚              â”‚  â”‚              â”‚  â”‚              â”‚            â”‚
â”‚  â”‚ - ç”Ÿæˆåºåˆ—   â”‚  â”‚ - è®¡ç®—ä»·å€¼   â”‚  â”‚ - è®¡ç®—å‚è€ƒ   â”‚            â”‚
â”‚  â”‚ - æ›´æ–°Actor â”‚  â”‚ - æ›´æ–°Critic â”‚  â”‚   log prob   â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. è®¾è®¡åŸåˆ™

1. **è®¡ç®—ä¸æ•°æ®è§£è€¦**ï¼šé€šè¿‡ `DataProto` åè®®ç»Ÿä¸€æ•°æ®äº¤æ¢æ ¼å¼
2. **çµæ´»çš„è®¾å¤‡æ˜ å°„**ï¼šæ”¯æŒå¤šç§æ¨¡å‹åœ¨ä¸åŒ GPU é›†ç¾¤ä¸Šçš„éƒ¨ç½²ç­–ç•¥
3. **æ¨¡å—åŒ–é›†æˆ**ï¼šæ— ç¼é›†æˆ PyTorch FSDPã€Megatron-LMã€vLLM ç­‰æ¡†æ¶
4. **æ··åˆå¼•æ“ (Hybrid Engine)**ï¼šè®­ç»ƒå’Œæ¨ç†å…±äº«æ¨¡å‹æƒé‡ï¼Œé€šè¿‡ 3D-HybridEngine å®ç°é«˜æ•ˆåˆ‡æ¢

---

## ğŸ“ ç›®å½•ç»“æ„è¯¦è§£

```
TinyZero/
â”œâ”€â”€ verl/                          # æ ¸å¿ƒæ¡†æ¶ä»£ç 
â”‚   â”œâ”€â”€ protocol.py                # æ•°æ®ä¼ è¾“åè®® (DataProto)
â”‚   â”œâ”€â”€ single_controller/         # å•æ§åˆ¶å™¨æ¨¡å¼å®ç°
â”‚   â”‚   â”œâ”€â”€ base/                  # åŸºç¡€æŠ½è±¡ç±»
â”‚   â”‚   â”‚   â”œâ”€â”€ decorator.py       # åˆ†å‘/æ‰§è¡Œè£…é¥°å™¨
â”‚   â”‚   â”‚   â”œâ”€â”€ worker.py          # Worker åŸºç±»
â”‚   â”‚   â”‚   â””â”€â”€ worker_group.py    # WorkerGroup ç®¡ç†
â”‚   â”‚   â””â”€â”€ ray/                   # Ray åç«¯å®ç°
â”‚   â”‚       â””â”€â”€ base.py            # RayWorkerGroup
â”‚   â”‚
â”‚   â”œâ”€â”€ trainer/                   # è®­ç»ƒå™¨
â”‚   â”‚   â”œâ”€â”€ ppo/                   # PPO ç®—æ³•
â”‚   â”‚   â”‚   â”œâ”€â”€ core_algos.py      # æ ¸å¿ƒç®—æ³• (GAE, KL, Policy Loss)
â”‚   â”‚   â”‚   â””â”€â”€ ray_trainer.py     # Ray åˆ†å¸ƒå¼ PPO Trainer
â”‚   â”‚   â””â”€â”€ main_ppo.py            # ä¸»å…¥å£
â”‚   â”‚
â”‚   â”œâ”€â”€ workers/                   # Worker å®ç°
â”‚   â”‚   â”œâ”€â”€ fsdp_workers.py        # FSDP åç«¯ Workers
â”‚   â”‚   â”œâ”€â”€ megatron_workers.py    # Megatron-LM åç«¯
â”‚   â”‚   â”œâ”€â”€ actor/                 # Actor æ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ critic/                # Critic æ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ rollout/               # æ¨ç†å¼•æ“
â”‚   â”‚   â”‚   â”œâ”€â”€ vllm_rollout/      # vLLM æ¨ç†
â”‚   â”‚   â”‚   â””â”€â”€ hf_rollout.py      # HuggingFace æ¨ç†
â”‚   â”‚   â””â”€â”€ sharding_manager/      # æƒé‡åˆ†ç‰‡ç®¡ç†
â”‚   â”‚       â”œâ”€â”€ fsdp_vllm.py       # FSDP â†” vLLM æƒé‡è½¬æ¢
â”‚   â”‚       â””â”€â”€ fsdp_ulysses.py    # FSDP + Ulysses SP
â”‚   â”‚
â”‚   â”œâ”€â”€ models/                    # æ¨¡å‹é€‚é…
â”‚   â””â”€â”€ utils/                     # å·¥å…·å‡½æ•°
â”‚       â”œâ”€â”€ reward_score/          # å¥–åŠ±è®¡ç®—
â”‚       â”‚   â”œâ”€â”€ countdown.py       # Countdown ä»»åŠ¡å¥–åŠ±
â”‚       â”‚   â””â”€â”€ gsm8k.py           # GSM8K ä»»åŠ¡å¥–åŠ±
â”‚       â””â”€â”€ dataset/               # æ•°æ®å¤„ç†
â”‚
â”œâ”€â”€ examples/                      # ç¤ºä¾‹ä»£ç 
â”‚   â”œâ”€â”€ data_preprocess/           # æ•°æ®é¢„å¤„ç†è„šæœ¬
â”‚   â”‚   â”œâ”€â”€ countdown.py           # Countdown æ•°æ®å‡†å¤‡
â”‚   â”‚   â””â”€â”€ gsm8k.py               # GSM8K æ•°æ®å‡†å¤‡
â”‚   â””â”€â”€ ppo_trainer/               # PPO è®­ç»ƒé…ç½®
â”‚
â”œâ”€â”€ scripts/                       # è¿è¡Œè„šæœ¬
â”‚   â””â”€â”€ train_tiny_zero.sh         # TinyZero è®­ç»ƒå…¥å£
â”‚
â””â”€â”€ tests/                         # æµ‹è¯•ä»£ç 
```

---

## ğŸ”„ æ ¸å¿ƒæ•°æ®æµæ¶æ„

### DataProto åè®®

`DataProto` æ˜¯ veRL ä¸­ç»Ÿä¸€çš„æ•°æ®äº¤æ¢åè®®ï¼Œç”¨äºåœ¨ä¸åŒç»„ä»¶é—´ä¼ é€’æ•°æ®ï¼š

```python
@dataclass
class DataProto:
    batch: TensorDict = None          # å¼ é‡æ•°æ® (PyTorch TensorDict)
    non_tensor_batch: Dict = {}       # éå¼ é‡æ•°æ® (numpy arrays)
    meta_info: Dict = {}              # å…ƒä¿¡æ¯

    # æ ¸å¿ƒæ–¹æ³•
    def chunk(self, chunks: int)      # åˆ†ç‰‡ (ç”¨äºæ•°æ®å¹¶è¡Œ)
    def concat(data: List)            # åˆå¹¶
    def union(self, other)            # åˆå¹¶ä¸¤ä¸ª DataProto
    def make_iterator(...)            # åˆ›å»ºè¿­ä»£å™¨
```

### PPO è®­ç»ƒæ•°æ®æµ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            PPO Training Loop                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. æ•°æ®å‡†å¤‡
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ DataLoader  â”‚â”€â”€â”€â”€â”€â–¶ prompts (input_ids, attention_mask)
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

2. ç”Ÿæˆé˜¶æ®µ (Rollout)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  ActorRolloutWorker.generate_sequences(prompts)              â”‚
   â”‚                                                               â”‚
   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
   â”‚  â”‚  ShardingManager                                        â”‚  â”‚
   â”‚  â”‚  FSDP Weights â”€â”€â–¶ vLLM Weights                         â”‚  â”‚
   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
   â”‚                          â”‚                                    â”‚
   â”‚                          â–¼                                    â”‚
   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
   â”‚  â”‚  vLLM Rollout                                           â”‚  â”‚
   â”‚  â”‚  - è‡ªå›å½’ç”Ÿæˆ responses                                  â”‚  â”‚
   â”‚  â”‚  - è¿”å› old_log_probs                                   â”‚  â”‚
   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
   output: {responses, old_log_probs, attention_mask}

3. å‚è€ƒç­–ç•¥è®¡ç®— (å¯é€‰)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  RefPolicyWorker.compute_ref_log_prob(data)                  â”‚
   â”‚  â”€â”€â–¶ ref_log_prob                                            â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

4. ä»·å€¼ä¼°è®¡
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  CriticWorker.compute_values(data)                           â”‚
   â”‚  â”€â”€â–¶ values                                                  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

5. å¥–åŠ±è®¡ç®— (Driver Process)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  RewardManager(data)                                         â”‚
   â”‚  - åŸºäºè§„åˆ™çš„å¥–åŠ± (countdown: æ–¹ç¨‹æ­£ç¡®æ€§)                      â”‚
   â”‚  - æˆ–æ¨¡å‹å¥–åŠ± (RewardModelWorker)                            â”‚
   â”‚  â”€â”€â–¶ token_level_scores                                      â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

6. ä¼˜åŠ¿ä¼°è®¡ (Driver Process - è½»é‡è®¡ç®—)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  apply_kl_penalty()    # KL æƒ©ç½š                            â”‚
   â”‚  compute_advantage()   # GAE æˆ– GRPO                        â”‚
   â”‚  â”€â”€â–¶ advantages, returns                                    â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

7. Critic æ›´æ–°
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  CriticWorker.update_critic(data)                           â”‚
   â”‚  - Value Loss = (V_pred - returns)Â²                         â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

8. Actor æ›´æ–°
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  ActorRolloutWorker.update_actor(data)                      â”‚
   â”‚  - PPO Clipped Policy Loss                                   â”‚
   â”‚  - Entropy Bonus (å¯é€‰)                                      â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§  æ ¸å¿ƒç®—æ³•å®ç°

### 1. PPO æ ¸å¿ƒç®—æ³• (`verl/trainer/ppo/core_algos.py`)

#### GAE (Generalized Advantage Estimation)

```python
def compute_gae_advantage_return(token_level_rewards, values, eos_mask, gamma, lam):
    """
    è®¡ç®— token çº§åˆ«çš„ advantage å’Œ returns
    
    ä¼˜åŠ¿ä¼°è®¡å…¬å¼ï¼š
    Î´_t = r_t + Î³ * V(s_{t+1}) - V(s_t)
    A_t = Î´_t + Î³Î» * A_{t+1}
    """
    for t in reversed(range(gen_len)):
        nextvalues = values[:, t + 1] if t < gen_len - 1 else 0.0
        delta = token_level_rewards[:, t] + gamma * nextvalues - values[:, t]
        lastgaelam = delta + gamma * lam * lastgaelam
        advantages_reversed.append(lastgaelam)
    
    advantages = torch.stack(advantages_reversed[::-1], dim=1)
    returns = advantages + values
    advantages = masked_whiten(advantages, eos_mask)  # å½’ä¸€åŒ–
    return advantages, returns
```

#### PPO Policy Loss

```python
def compute_policy_loss(old_log_prob, log_prob, advantages, eos_mask, cliprange):
    """
    PPO Clipped Objective
    
    L^{CLIP}(Î¸) = E[min(r(Î¸)A, clip(r(Î¸), 1-Îµ, 1+Îµ)A)]
    å…¶ä¸­ r(Î¸) = Ï€_Î¸(a|s) / Ï€_Î¸_old(a|s)
    """
    ratio = torch.exp(log_prob - old_log_prob)
    pg_losses = -advantages * ratio
    pg_losses2 = -advantages * torch.clamp(ratio, 1.0 - cliprange, 1.0 + cliprange)
    pg_loss = masked_mean(torch.max(pg_losses, pg_losses2), eos_mask)
    return pg_loss, pg_clipfrac, ppo_kl
```

#### GRPO (Group Relative Policy Optimization)

```python
def compute_grpo_outcome_advantage(token_level_rewards, eos_mask, index):
    """
    GRPO: åŸºäºç»„å†…ç›¸å¯¹å¥–åŠ±è®¡ç®— advantage
    
    å¯¹äºç›¸åŒ prompt çš„å¤šä¸ª response:
    advantage = (score - mean(group_scores)) / std(group_scores)
    """
    # æŒ‰ prompt index åˆ†ç»„è®¡ç®— mean å’Œ std
    for idx in id2score:
        id2mean[idx] = torch.mean(torch.tensor(id2score[idx]))
        id2std[idx] = torch.std(torch.tensor([id2score[idx]]))
    
    # å½’ä¸€åŒ–
    scores[i] = (scores[i] - id2mean[index[i]]) / (id2std[index[i]] + epsilon)
```

### 2. KL æ§åˆ¶å™¨

```python
class AdaptiveKLController:
    """
    è‡ªé€‚åº” KL ç³»æ•°æ§åˆ¶å™¨
    æ ¹æ®å½“å‰ KL æ•£åº¦åŠ¨æ€è°ƒæ•´æƒ©ç½šç³»æ•°
    """
    def update(self, current_kl, n_steps):
        proportional_error = np.clip(current_kl / self.target - 1, -0.2, 0.2)
        mult = 1 + proportional_error * n_steps / self.horizon
        self.value *= mult
```

---

## ğŸ”§ Worker ç³»ç»Ÿè®¾è®¡

### 1. Worker è§’è‰²å®šä¹‰

```python
class Role(Enum):
    Actor = 0           # ç­–ç•¥ç½‘ç»œ
    Rollout = 1         # æ¨ç†å¼•æ“
    ActorRollout = 2    # Actor + Rollout æ··åˆ
    Critic = 3          # ä»·å€¼ç½‘ç»œ
    RefPolicy = 4       # å‚è€ƒç­–ç•¥
    RewardModel = 5     # å¥–åŠ±æ¨¡å‹
    ActorRolloutRef = 6 # Actor + Rollout + Ref æ··åˆ
```

### 2. Dispatch æ¨¡å¼

veRL é€šè¿‡è£…é¥°å™¨ç³»ç»Ÿå®ç°çµæ´»çš„æ•°æ®åˆ†å‘å’Œæ”¶é›†ï¼š

```python
class Dispatch:
    RANK_ZERO = 0        # åªåœ¨ rank 0 æ‰§è¡Œ
    ONE_TO_ALL = 1       # å¹¿æ’­åˆ°æ‰€æœ‰ worker
    ALL_TO_ALL = 2       # å…¨å¯¹å…¨é€šä¿¡
    DP_COMPUTE = 8       # æ•°æ®å¹¶è¡Œè®¡ç®—
    DP_COMPUTE_PROTO = 9 # æ•°æ®å¹¶è¡Œ + DataProto è‡ªåŠ¨åˆ†ç‰‡
```

ä½¿ç”¨ç¤ºä¾‹ï¼š

```python
class ActorRolloutRefWorker(Worker):
    
    @register(dispatch_mode=Dispatch.ONE_TO_ALL)
    def init_model(self):
        """å¹¿æ’­åˆå§‹åŒ–å‘½ä»¤åˆ°æ‰€æœ‰ worker"""
        ...
    
    @register(dispatch_mode=Dispatch.DP_COMPUTE_PROTO)
    def generate_sequences(self, prompts: DataProto):
        """
        è‡ªåŠ¨å°† prompts æŒ‰ batch ç»´åº¦åˆ†ç‰‡åˆ°å„ä¸ª workerï¼Œ
        æ‰§è¡Œåè‡ªåŠ¨åˆå¹¶ç»“æœ
        """
        ...
    
    @register(dispatch_mode=Dispatch.DP_COMPUTE_PROTO)
    def update_actor(self, data: DataProto):
        """åˆ†å¸ƒå¼ç­–ç•¥æ›´æ–°"""
        ...
```

### 3. FSDP Worker å®ç°

`ActorRolloutRefWorker` æ˜¯æ ¸å¿ƒçš„å¤šåŠŸèƒ½ Workerï¼Œå¯ä»¥æ ¹æ®é…ç½®æ‰®æ¼”ä¸åŒè§’è‰²ï¼š

```python
class ActorRolloutRefWorker(Worker):
    def __init__(self, config: DictConfig, role: str):
        # è§’è‰²åˆ¤æ–­
        self._is_actor = role in ['actor', 'actor_rollout', 'actor_rollout_ref']
        self._is_rollout = role in ['rollout', 'actor_rollout', 'actor_rollout_ref']
        self._is_ref = role in ['ref', 'actor_rollout_ref']
        
        # åˆå§‹åŒ– FSDP Device Mesh
        self.device_mesh = init_device_mesh('cuda', (world_size,), ['fsdp'])
        
        # åˆå§‹åŒ– Ulysses åºåˆ—å¹¶è¡Œ (å¯é€‰)
        if self.ulysses_sequence_parallel_size > 1:
            self.ulysses_device_mesh = init_device_mesh('cuda', (dp, sp), ['dp', 'sp'])
```

---

## âš¡ Hybrid Engine æ··åˆå¼•æ“

### æ ¸å¿ƒæ¦‚å¿µ

æ··åˆå¼•æ“çš„ç›®æ ‡æ˜¯è®©åŒä¸€å¥—æ¨¡å‹æƒé‡åœ¨ **è®­ç»ƒ** å’Œ **æ¨ç†** ä¹‹é—´é«˜æ•ˆåˆ‡æ¢ï¼š

```
è®­ç»ƒé˜¶æ®µ (FSDP)                      æ¨ç†é˜¶æ®µ (vLLM)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FSDP Sharded       â”‚             â”‚  vLLM TP Sharded    â”‚
â”‚  Parameters         â”‚ â—€â”€â”€â”€â”€â”€â”€â”€â–¶  â”‚  Parameters + KV    â”‚
â”‚                     â”‚  Resharding â”‚  Cache              â”‚
â”‚  - Full Shard       â”‚             â”‚  - Tensor Parallel  â”‚
â”‚  - Mixed Precision  â”‚             â”‚  - Paged Attention  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ShardingManager

`FSDPVLLMShardingManager` è´Ÿè´£ FSDP å’Œ vLLM ä¹‹é—´çš„æƒé‡åŒæ­¥ï¼š

```python
class FSDPVLLMShardingManager(BaseShardingManager):
    def __enter__(self):
        """è¿›å…¥æ¨ç†æ¨¡å¼"""
        # 1. ä» FSDP æ”¶é›†å®Œæ•´æƒé‡
        # 2. æ ¹æ® vLLM TP ç­–ç•¥é‡æ–°åˆ†ç‰‡
        # 3. åŠ è½½åˆ° vLLM å¼•æ“
        self.inference_engine.sync_model_weights(...)
    
    def __exit__(self, ...):
        """é€€å‡ºæ¨ç†æ¨¡å¼"""
        # é‡Šæ”¾ä¸´æ—¶ç¼“å­˜ï¼Œæ¢å¤ FSDP çŠ¶æ€
```

---

## ğŸ“Š TinyZero ç‰¹å®šåŠŸèƒ½

### 1. Countdown ä»»åŠ¡

ç›®æ ‡ï¼šç»™å®šç›®æ ‡æ•°å­—å’Œä¸€ç»„æ•°å­—ï¼Œæ‰¾å‡ºèƒ½å¾—åˆ°ç›®æ ‡çš„ç®—æœ¯è¡¨è¾¾å¼ã€‚

#### æ•°æ®æ ¼å¼

```python
# examples/data_preprocess/countdown.py
prompt = f"""Using the numbers {numbers}, create an equation that equals {target}. 
You can use basic arithmetic operations (+, -, *, /) and each number can only be used once.
Show your work in <think> </think> tags. 
And return the final answer in <answer> </answer> tags."""
```

#### å¥–åŠ±å‡½æ•°

```python
# verl/utils/reward_score/countdown.py
def compute_score(solution_str, ground_truth):
    """
    å¥–åŠ±è§„åˆ™ï¼š
    - æ— æ³•æå–ç­”æ¡ˆ: 0
    - æ ¼å¼æ­£ç¡®ä½†ç­”æ¡ˆé”™è¯¯: 0.1 (format_score)
    - ç­”æ¡ˆæ­£ç¡®: 1.0
    """
    equation = extract_solution(solution_str)  # ä» <answer> æ ‡ç­¾æå–
    
    if not validate_equation(equation, numbers):  # éªŒè¯ä½¿ç”¨çš„æ•°å­—
        return format_score
    
    result = evaluate_equation(equation)  # å®‰å…¨åœ°è®¡ç®—ç»“æœ
    if abs(result - target) < 1e-5:
        return 1.0
    else:
        return format_score
```

### 2. è®­ç»ƒé…ç½®å…³é”®å‚æ•°

```bash
# scripts/train_tiny_zero.sh

# æ•°æ®é…ç½®
data.train_batch_size=256
data.max_prompt_length=256
data.max_response_length=1024

# Actor é…ç½®
actor_rollout_ref.actor.optim.lr=1e-6
actor_rollout_ref.actor.ppo_mini_batch_size=64
actor_rollout_ref.actor.ppo_micro_batch_size=8

# Rollout é…ç½® (vLLM)
actor_rollout_ref.rollout.tensor_model_parallel_size=$ROLLOUT_TP_SIZE
actor_rollout_ref.rollout.gpu_memory_utilization=0.4

# Critic é…ç½®
critic.optim.lr=1e-5
critic.model.path=$BASE_MODEL  # ä½¿ç”¨ç›¸åŒçš„é¢„è®­ç»ƒæ¨¡å‹

# KL æƒ©ç½š
algorithm.kl_ctrl.kl_coef=0.001

# è®­ç»ƒè®¾ç½®
trainer.total_epochs=15
trainer.save_freq=100
trainer.test_freq=100
```

---

## ğŸŒ åˆ†å¸ƒå¼è®­ç»ƒæ¶æ„

### èµ„æºæ± ç®¡ç†

```python
@dataclass
class ResourcePoolManager:
    """
    ç®¡ç† GPU èµ„æºåˆ†é…
    """
    resource_pool_spec: dict[str, list[int]]  # pool_name -> [æ¯ä¸ªèŠ‚ç‚¹çš„ GPU æ•°]
    mapping: dict[Role, str]                   # Role -> pool_name
    
    def create_resource_pool(self):
        for name, process_on_nodes in self.resource_pool_spec.items():
            resource_pool = RayResourcePool(
                process_on_nodes=process_on_nodes,
                use_gpu=True,
                max_colocate_count=1  # FSDP: åˆå¹¶åˆ°ä¸€ä¸ªè¿›ç¨‹ç»„
            )
            self.resource_pool_dict[name] = resource_pool
```

### å…±ç½® Worker (Colocated Workers)

TinyZero ä½¿ç”¨å…±ç½®ç­–ç•¥ï¼Œå°† Actorã€Rolloutã€Ref Policy æ”¾åœ¨åŒä¸€ç»„ GPU ä¸Šï¼š

```python
# verl/trainer/ppo/ray_trainer.py
def init_workers(self):
    # æ‰€æœ‰æ¨¡å‹å…±äº«åŒä¸€ä¸ªèµ„æºæ± 
    mapping = {
        Role.ActorRollout: 'global_pool',
        Role.Critic: 'global_pool',
        Role.RefPolicy: 'global_pool',
    }
    
    # åˆ›å»ºå…±ç½® Worker ç±»
    worker_dict_cls = create_colocated_worker_cls(class_dict={
        'actor_rollout': actor_rollout_cls,
        'critic': critic_cls,
        'ref': ref_policy_cls,
    })
```

---

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–æŠ€æœ¯

### 1. åŠ¨æ€ Batch Size

```python
actor.use_dynamic_bsz = True  # æ ¹æ®åºåˆ—é•¿åº¦åŠ¨æ€è°ƒæ•´ batch
```

### 2. åºåˆ—é•¿åº¦å‡è¡¡

```python
# verl/utils/seqlen_balancing.py
def get_seqlen_balanced_partitions(seqlen_list, k_partitions, equal_size=True):
    """
    å°†æ•°æ®æŒ‰åºåˆ—é•¿åº¦å‡è¡¡åˆ†é…åˆ°å„ä¸ª DP rankï¼Œ
    é¿å…é•¿åºåˆ—å¯¼è‡´çš„è®¡ç®—è´Ÿè½½ä¸å‡
    """
```

### 3. å‚æ•°/æ¢¯åº¦/ä¼˜åŒ–å™¨å¸è½½

```python
# æ”¯æŒå°† FSDP å‚æ•°å¸è½½åˆ° CPU ä»¥èŠ‚çœ GPU æ˜¾å­˜
actor.fsdp_config.param_offload = True
actor.fsdp_config.grad_offload = True
actor.fsdp_config.optimizer_offload = True
```

### 4. Gradient Checkpointing

```python
# å‡å°‘æ˜¾å­˜ä½¿ç”¨
critic.model.enable_gradient_checkpointing = True
```

---

## ğŸ”— æ‰©å±•æŒ‡å—

### æ·»åŠ æ–°ä»»åŠ¡

1. **åˆ›å»ºæ•°æ®é¢„å¤„ç†è„šæœ¬** (`examples/data_preprocess/your_task.py`)
2. **å®ç°å¥–åŠ±å‡½æ•°** (`verl/utils/reward_score/your_task.py`)
3. **åœ¨ `main_ppo.py` ä¸­æ³¨å†Œ**ï¼š

```python
def _select_rm_score_fn(data_source):
    if "your_task" in data_source:
        return your_task.compute_score
```

### æ·»åŠ æ–°ç®—æ³•

1. **å®ç°æ ¸å¿ƒç®—æ³•** (`verl/trainer/your_algo/core_algos.py`)
2. **åˆ›å»º Trainer** (`verl/trainer/your_algo/ray_trainer.py`)
3. **ç»§æ‰¿å¹¶ä¿®æ”¹ Worker** è¡Œä¸º

---

## ğŸ“š å‚è€ƒèµ„æ–™

- [HybridFlow è®ºæ–‡](https://arxiv.org/abs/2409.19256v2)
- [veRL å®˜æ–¹æ–‡æ¡£](https://verl.readthedocs.io/)
- [TinyZero å®éªŒæ—¥å¿—](https://wandb.ai/jiayipan/TinyZero)
- [DeepSeek R1 è®ºæ–‡](https://github.com/deepseek-ai/DeepSeek-R1)

---

## ğŸ’¡ å…³é”®æ´å¯Ÿ

### ä¸ºä»€ä¹ˆ TinyZero èƒ½å·¥ä½œï¼Ÿ

1. **è‡ªä¸»å‘å±•æ¨ç†èƒ½åŠ›**ï¼šé€šè¿‡ RL è®­ç»ƒï¼Œæ¨¡å‹å­¦ä¼šåœ¨ `<think>` æ ‡ç­¾ä¸­è¿›è¡Œå¤šæ­¥æ¨ç†
2. **ç®€å•ä½†æœ‰æ•ˆçš„å¥–åŠ±**ï¼šåªåŸºäºæœ€ç»ˆç­”æ¡ˆçš„æ­£ç¡®æ€§ï¼Œä¸éœ€è¦è¿‡ç¨‹ç›‘ç£
3. **å……åˆ†çš„æ¢ç´¢**ï¼šé•¿è¾¾ 1024 token çš„ response é•¿åº¦å…è®¸æ¨¡å‹å°è¯•å¤šç§æ¨ç†è·¯å¾„
4. **ç¨³å®šçš„ KL çº¦æŸ**ï¼šé˜²æ­¢æ¨¡å‹åç¦»é¢„è®­ç»ƒåˆ†å¸ƒå¤ªè¿œ

### æ¶æ„ä¼˜åŠ¿

1. **é«˜ååé‡**ï¼šé€šè¿‡ vLLM åŠ é€Ÿæ¨ç†ï¼ŒFSDP ä¼˜åŒ–è®­ç»ƒ
2. **å†…å­˜æ•ˆç‡**ï¼šæ··åˆå¼•æ“é¿å…äº†å†—ä½™çš„æ¨¡å‹å‰¯æœ¬
3. **æ˜“äºæ‰©å±•**ï¼šæ¨¡å—åŒ–è®¾è®¡ä½¿å¾—æ·»åŠ æ–°ä»»åŠ¡å’Œç®—æ³•å˜å¾—ç®€å•
