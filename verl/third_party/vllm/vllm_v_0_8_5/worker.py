# Copyright 2024 Bytedance Ltd. and/or its affiliates
# Copyright 2023 The vLLM team.
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# Adapted from https://github.com/vllm-project/vllm/blob/main/vllm/worker/worker.py
#
# NOTE: Adapted for vLLM 0.8.5 - Uses VllmConfig instead of separate config objects
"""A GPU worker class."""
import gc
import os
from typing import Dict, List, Optional, Tuple, Type, Union

import torch
import torch.distributed
import torch.nn as nn
from vllm.config import VllmConfig
from vllm.distributed import get_tensor_model_parallel_group, init_distributed_environment, set_custom_all_reduce
from vllm.model_executor import set_random_seed
from vllm.model_executor.layers.sampler import SamplerOutput
from vllm.sequence import ExecuteModelRequest, IntermediateTensors
from vllm.worker.cache_engine import CacheEngine
from vllm.worker.model_runner import GPUModelRunnerBase
from vllm.worker.model_runner_base import ModelRunnerInputBase
from vllm.worker.worker_base import WorkerInput

from .config import LoadFormat
from .dtensor_weight_loaders import load_dtensor_weights
from .hf_weight_loader import load_hf_weights
from .megatron_weight_loaders import load_megatron_weights
from .model_runner import ModelRunner
from .parallel_state import ensure_model_parallel_initialized

from vllm.logger import init_logger

logger = init_logger(__name__)


def _check_if_gpu_supports_dtype(torch_dtype: torch.dtype):
    if torch_dtype == torch.bfloat16:
        compute_capability = torch.cuda.get_device_capability()
        if compute_capability[0] < 8:
            gpu_name = torch.cuda.get_device_name()
            raise ValueError(
                "Bfloat16 is only supported on GPUs with compute capability "
                f"of at least 8.0. Your {gpu_name} GPU has compute capability "
                f"{compute_capability[0]}.{compute_capability[1]}. "
                "You can use float16 instead by explicitly setting the "
                "`dtype` flag in CLI, for example: --dtype=half."
            )


class Worker:
    """A worker class that executes (a partition of) the model on a GPU.
    
    Adapted for vLLM 0.8.5 API using VllmConfig.
    """

    def __init__(
        self,
        model: Union[nn.Module, Dict],  # model itself or its parameter dict
        vllm_config: VllmConfig,
        local_rank: int,
        rank: int,
        distributed_init_method: str,
        is_driver_worker: bool = False,
        model_runner_cls: Optional[Type[GPUModelRunnerBase]] = None,
    ) -> None:
        self.vllm_config = vllm_config
        
        # Extract individual configs for convenience
        self.model_config = vllm_config.model_config
        self.parallel_config = vllm_config.parallel_config
        self.scheduler_config = vllm_config.scheduler_config
        self.device_config = vllm_config.device_config
        self.cache_config = vllm_config.cache_config
        self.load_config = vllm_config.load_config
        self.lora_config = vllm_config.lora_config
        self.speculative_config = vllm_config.speculative_config
        self.prompt_adapter_config = vllm_config.prompt_adapter_config
        
        self.parallel_config.rank = rank
        self.local_rank = local_rank
        self.rank = rank
        self.distributed_init_method = distributed_init_method
        self.is_driver_worker = is_driver_worker

        if self.model_config.trust_remote_code:
            from vllm.utils import init_cached_hf_modules
            init_cached_hf_modules()

        # Return hidden states from target model if the draft model is an mlp_speculator
        speculative_args = {}
        if self.speculative_config is not None:
            draft_hf_config = self.speculative_config.draft_model_config.hf_config
            if hasattr(draft_hf_config, 'model_type'):
                if draft_hf_config.model_type in ["medusa", "mlp_speculator", "eagle", "deepseek_mtp"]:
                    speculative_args = {"return_hidden_states": True}

        # Initialize model runner
        ModelRunnerClass: Type[GPUModelRunnerBase] = ModelRunner
        if model_runner_cls is not None:
            ModelRunnerClass = model_runner_cls
        
        self.model_runner: GPUModelRunnerBase = ModelRunnerClass(
            model=model,
            vllm_config=vllm_config,
            kv_cache_dtype=self.cache_config.cache_dtype,
            is_driver_worker=is_driver_worker,
            **speculative_args,
        )

        # Uninitialized cache engine. Will be initialized by initialize_cache
        self.cache_engine: List[CacheEngine] = None
        self.gpu_cache: Optional[List[List[torch.Tensor]]] = None
        self.kv_cache = None  # Alias for compatibility

        # For offloading inference engine params
        self.cpu_model = None

    def init_device(self) -> None:
        if self.device_config.device.type == "cuda":
            os.environ["TORCH_NCCL_AVOID_RECORD_STREAMS"] = "1"

            self.rank = self.rank if self.rank is not None else int(os.getenv("RANK", "-1"))
            local_rank = int(os.getenv("LOCAL_RANK", "0"))
            self.device = torch.device(f"cuda:{local_rank}")
            if self.rank < 0:
                raise ValueError("Invalid or unspecified rank.")
            torch.cuda.set_device(self.device)

            world_size = int(os.getenv("WORLD_SIZE", "-1"))
            assert world_size != -1, "The world_size is set to -1, not initialized by TORCHRUN"
            self.parallel_config.world_size = world_size

            _check_if_gpu_supports_dtype(self.model_config.dtype)
            torch.cuda.empty_cache()
            self.init_gpu_memory = torch.cuda.mem_get_info()[0]
        else:
            raise RuntimeError(f"Not support device type: {self.device_config.device}")

        # Initialize the distributed environment
        init_worker_distributed_environment(
            self.parallel_config, self.rank, self.distributed_init_method, self.local_rank
        )
        set_random_seed(self.model_config.seed)

    def load_model(self):
        """Load the model."""
        self.model_runner.load_model()

    @torch.inference_mode()
    def determine_num_available_blocks(self) -> Tuple[int, int]:
        """Profiles peak memory usage to determine KV blocks."""
        torch.cuda.empty_cache()

        self.model_runner.profile_run()

        torch.cuda.synchronize()
        free_gpu_memory, total_gpu_memory = torch.cuda.mem_get_info()
        peak_memory = total_gpu_memory - free_gpu_memory

        assert peak_memory > 0, (
            "Error in memory profiling. GPU memory was not properly cleaned up."
        )

        cache_block_size = self.get_cache_block_size_bytes()

        num_gpu_blocks = int(
            (free_gpu_memory * self.cache_config.gpu_memory_utilization) // cache_block_size
        )
        num_cpu_blocks = int(self.cache_config.swap_space_bytes // cache_block_size)
        num_gpu_blocks = max(num_gpu_blocks, 0)
        num_cpu_blocks = max(num_cpu_blocks, 0)

        if hasattr(self.model_runner, 'lora_manager') and self.model_runner.lora_manager:
            self.model_runner.remove_all_loras()

        # Synchronize number of blocks across all ranks
        num_gpu_blocks = torch.tensor([num_gpu_blocks], device="cuda")
        num_cpu_blocks = torch.tensor([num_cpu_blocks], device="cuda")

        torch.distributed.all_reduce(
            num_gpu_blocks, op=torch.distributed.ReduceOp.MIN,
            group=get_tensor_model_parallel_group().device_group
        )
        torch.distributed.all_reduce(
            num_cpu_blocks, op=torch.distributed.ReduceOp.MIN,
            group=get_tensor_model_parallel_group().device_group
        )

        num_gpu_blocks = num_gpu_blocks.item()
        num_cpu_blocks = num_cpu_blocks.item()
        gc.collect()
        torch.cuda.empty_cache()
        return num_gpu_blocks, num_cpu_blocks

    def initialize_cache(self, num_gpu_blocks: int, num_cpu_blocks: int) -> None:
        """Initialize the KV cache."""
        self.cache_config.num_gpu_blocks = num_gpu_blocks
        self.cache_config.num_cpu_blocks = num_cpu_blocks
        self._init_cache_engine()
        self._warm_up_model()

    def _init_cache_engine(self):
        if self.cache_engine is None and self.gpu_cache is None:
            assert self.cache_config.num_gpu_blocks is not None
            self.cache_engine = [
                CacheEngine(
                    self.cache_config, self.model_config,
                    self.parallel_config, self.device_config
                )
                for _ in range(self.parallel_config.pipeline_parallel_size)
            ]
            self.gpu_cache = [
                self.cache_engine[ve].gpu_cache
                for ve in range(self.parallel_config.pipeline_parallel_size)
            ]
            self.kv_cache = self.gpu_cache

    def _warm_up_model(self) -> None:
        """Warm up model and capture CUDA graphs if needed."""
        if not self.model_config.enforce_eager:
            self.model_runner.capture_model(self.gpu_cache)
        set_random_seed(self.model_config.seed)

    def free_cache_engine(self):
        self.cache_engine = None
        self.gpu_cache = None
        self.kv_cache = None

    def get_cache_block_size_bytes(self) -> int:
        return CacheEngine.get_cache_block_size(
            self.cache_config, self.model_config, self.parallel_config
        )

    @torch.inference_mode()
    def prepare_worker_input(self, execute_model_req: ExecuteModelRequest) -> WorkerInput:
        virtual_engine = execute_model_req.virtual_engine
        num_steps = execute_model_req.num_steps
        num_seq_groups = len(execute_model_req.seq_group_metadata_list)

        blocks_to_swap_in = torch.tensor(
            execute_model_req.blocks_to_swap_in, device="cpu", dtype=torch.int64
        ).view(-1, 2)
        blocks_to_swap_out = torch.tensor(
            execute_model_req.blocks_to_swap_out, device="cpu", dtype=torch.int64
        ).view(-1, 2)
        blocks_to_copy = torch.tensor(
            execute_model_req.blocks_to_copy, device=self.device, dtype=torch.int64
        ).view(-1, 2)

        return WorkerInput(
            num_seq_groups=num_seq_groups,
            blocks_to_swap_in=blocks_to_swap_in,
            blocks_to_swap_out=blocks_to_swap_out,
            blocks_to_copy=blocks_to_copy,
            virtual_engine=virtual_engine,
            num_steps=num_steps,
        )

    @torch.inference_mode()
    def execute_worker(self, worker_input: WorkerInput) -> None:
        virtual_engine = worker_input.virtual_engine
        if worker_input.blocks_to_swap_in is not None and worker_input.blocks_to_swap_in.numel() > 0:
            self.cache_engine[virtual_engine].swap_in(worker_input.blocks_to_swap_in)
        if worker_input.blocks_to_swap_out is not None and worker_input.blocks_to_swap_out.numel() > 0:
            self.cache_engine[virtual_engine].swap_out(worker_input.blocks_to_swap_out)
        if worker_input.blocks_to_copy is not None and worker_input.blocks_to_copy.numel() > 0:
            self.cache_engine[virtual_engine].copy(worker_input.blocks_to_copy)

    def execute_model(
        self,
        execute_model_req: ExecuteModelRequest,
        intermediate_tensors: Optional[IntermediateTensors] = None
    ) -> Optional[List[SamplerOutput]]:
        """Execute model in SPMD fashion."""
        assert execute_model_req is not None

        worker_input: WorkerInput = self.prepare_worker_input(execute_model_req=execute_model_req)
        model_input: ModelRunnerInputBase = self.model_runner.prepare_model_input(
            execute_model_req.seq_group_metadata_list
        )

        self.execute_worker(worker_input)

        if worker_input.num_seq_groups == 0:
            return []

        return self.model_runner.execute_model(
            model_input,
            self.kv_cache[worker_input.virtual_engine] if self.kv_cache is not None else None,
            intermediate_tensors,
        )

    def sync_model_weights(self, actor_weights: Dict, load_format: str):
        if load_format in [LoadFormat.MEGATRON, LoadFormat.AUTO]:
            load_megatron_weights(actor_weights, self.model_runner.model)
        elif load_format == LoadFormat.HF:
            load_hf_weights(actor_weights, self.model_runner.model)
        elif load_format == LoadFormat.DTENSOR:
            load_dtensor_weights(actor_weights, self.model_runner.model)

    def offload_model_weights(self) -> None:
        if self.cpu_model is None:
            self.cpu_model = {}
            for name, params in self.model_runner.model.named_parameters():
                self.cpu_model[name] = torch.empty_like(params, device="cpu")
                params.data = self.cpu_model[name]
        else:
            for name, params in self.model_runner.model.named_parameters():
                params.data = self.cpu_model[name]


def init_worker_distributed_environment(
    parallel_config,
    rank: int,
    distributed_init_method: Optional[str] = "env://",
    local_rank: int = -1,
) -> None:
    """Initialize the distributed environment."""
    set_custom_all_reduce(not parallel_config.disable_custom_all_reduce)
    init_distributed_environment(parallel_config.world_size, rank, distributed_init_method, local_rank)
    ensure_model_parallel_initialized(
        tensor_model_parallel_size=parallel_config.tensor_parallel_size,
        pipeline_model_parallel_size=parallel_config.pipeline_parallel_size,
    )
    torch.distributed.all_reduce(torch.zeros(1).cuda())
